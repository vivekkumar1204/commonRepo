Spark with jupyter notebook:
----------------------------
conda install -c conda-forge findspark

import findspark
findspark.init()
findspark.find()
import pyspark
findspark.find()

from pyspark.sql import SparkSession

****************************************************Spark SQL*************************************************
DataFrames & DatSets:
---------------------
1: Creating DataFrames
2: Untyped Dataset Operations (aka DataFrame Operations)
3: Running SQL Queries Programmatically
4: Global Temporary View

5: Creating Datasets --> Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.

6: Interoperating with RDDs
> Inferring the Schema Using Reflection --> Spark SQL using two ways to create dataset from existing RDD: 1- Use Reflection to infer schema of RDD, 2- Create schema programatically and apply while creating the dataset.
> Programmatically Specifying the Schema
7: Aggregations
> Untyped User-Defined Aggregate Functions
> Type-Safe User-Defined Aggregate Functions
8: Data Sources
> Generic Load/Save Functions
 > Manually Specifying Options --> Dataset<Row> peopleJsonDF = spark.read().format("json").load("C:\\\\Users\\\\vivek.kumar\\\\Desktop\\\\Project\\\\DataSets\\\\spark\\\\Doc_example\\\\people.json");
 peopleJsonDF.write().format("parquet").save("C:\\\\Users\\\\vivek.kumar\\\\Desktop\\\\Project\\\\DataSets\\\\spark\\\\Doc_example\\\\peoplejsonparquet");
 > Run SQL on files directly --> spark.sql("select * from parquet.`C:\\Users\\vivek.kumar\\Desktop\\Project\\DataSets\\spark\\Doc_example\\users.parquet`").show
 
 > Save Modes --> Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an Overwrite, the data will be deleted before writing out the new data.
 
 > Saving to Persistent Tables --> DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you. Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table.
 Note: To repair metdata of hive table: MSCK REPAIR 
 
 > Bucketing, Sorting and Partitioning
 
9: Parquet Files --> Parquet is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.

> Loading Data Programmatically

> Partition Discovery

> Schema Merging

> Hive metastore Parquet table conversion

 > Hive/Parquet Schema Reconciliation
 
 > Metadata Refreshing
 
10: Configuration

11: JSON Datasets

12: Hive Tables

13: Specifying storage format for Hive tables

14: Interacting with Different Versions of Hive Metastore

15: JDBC To Other Databases

16: Troubleshooting

17: Performance Tuning

> Caching Data In Memory

> Other Configuration Options

18: Distributed SQL Engine

> Running the Thrift JDBC/ODBC 
> Running the Spark SQL CLI
-----------------------------------------------------------
Note: We can create DF&DS from existing RDD, Hive tables, external sources.

DataFrame: It is a distributed collection of data organized into named columns.It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.

DatSet: A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).

Note: As mentioned above, in Spark 2.0, DataFrames are just Dataset of Rows in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.

Dataset creation:
> With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.

1: Dataset creation from existing RDD:
-----------------------------------
> Infering the schema using reflection --> Create RDD from file of java bean type i.e Person --> pass through map() function and set the values into Person objects --> create dataset of Row type by using rdd and Person.class as schema.

> Programmatically specifying the schema --> Create RDD from file of simple String type --> Pass schemastring into a string type variable --> create StructType schema by passing List of StructField into create structfield() method to create StructType schema --> Create Row type rdd by passing string type rdd through map using RowFactory class --> then call createdataframe method to create dataframe of Row type.

Note: While creating dataset , it needs seralized reflection of schema i.e encoded reflection.

2: Dataset creation from Hive or RDBMS tables:
----------------------------------------------

3: Dataset creation from spark data sources(files):
---------------------------------------------------

TempView: 
> we can create temp or global view from dataframe or dataset.
> Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1.

Encoders: Encoders are used to serialize the objects to process and store over the network.While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.

Optimization techniques: 
------------------------
Catalyst: It is a functional query optimization framework. It supports both rule based and cost based optimizatio techniques. We use Catalyst's general transformation in four phases:
1: Analysis : Spark SQL uses Catalyst rules and a Catalog object that tracks the tables in all data sources to resolve these attributes. 
2: Logical optimization : The logical optimization phase applies standard rule-based optimizations to the logical plan. And Cost-based optimization is performed by generating multiple plans using rules, and then computing their costs. These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, and other rules.
3: Physical planning : Spark SQL takes a logical plan and generates one or more physical plans, using physical operators that match the Spark execution engine. It then selects a plan using a cost model.At the moment, cost-based optimization is only used to select join algorithms: for relations that are known to be small, Spark SQL uses a broadcast join, using a peer-to-peer broadcast facility available in Spark. 
4: Code Generation : The final phase of query optimization involves generating Java bytecode to run on each machine.

Trees: 
> Tree contains node objects.
> Objects are immutable in nature.

Rules: We can manipulate tree using rules.
> we can define rules as function from one tree to another tree.

> 1: Rule based optimization(Catalyst is a modular library)

> 2: Cost based optimization 

Performance tuning:
-------------------
> Spark SQL can cache table by using an in-memory columnar format by calling spark.catalog.cacheTable("tableName") or dataframe.cache(). Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure.
Some properties:
spark.sql.inMemoryColumnarStorage.compressed = true
spark.sql.inMemoryColumnarStorage.batchSize = 10000
	
Four step update strategy with spark:
-------------------------------------

--# Hie update: 4 step update strategy
> Base table

--1: ingest data
drop table if exists base_tbl;
create table base_tbl(id int, field1 string, field2 string, modified_date string) row format delimited fields terminated by ',';
--Data:
insert into table base_tbl values(1, 'abcdef', 'efgh', '2014-03-01 09:22:52'),
(2, 'abc', 'efgh', '2014-03-01 09:22:52'),
(3, 'abcde', 'efgh', '2014-02-01 09:22:52');

drop table if exists inc_tbl;
create table inc_tbl(id int, field1 string, field2 string, modified_date string) row format delimited fields terminated by ',';
--Data:

insert into table inc_tbl values(1, 'abcdef', 'efgh', '2014-04-01 09:22:52'),
(2, 'abc', 'efgh', '2014-04-01 09:22:52'),
(3, 'abcde', 'e', '2014-03-01 10:22:52'),
(4, 'ab', 'ef', '2014-04-01 09:22:52'),
(5, 'abd', 'gh', '2014-05-01 09:22:52');


--2: Create Reconcile View
drop view if exists reconcile_view;
create view reconcile_view as
select t1.* from 
(select * from base_tbl
union all
select * from inc_tbl) t1
join
(select id, max(modified_date) max_modified from 
(select * from base_tbl
union all
select * from inc_tbl) t2
group by id)s
on t1.id == s.id 
and t1.modified_date = s.max_modified;

--3: Compact the view data into table
drop table if exists compact_tbl;
create table compact_tbl as
select * from reconcile_view;

--4: Purge the records into base table
drop table if exists base_tbl;
create table base_tbl as
select * from compact_tbl;

***************************************Spark: The Definitive Guide**************************************
Basic Structured Operations:
----------------------------
#Columns As Expression:
> expr("somecol - 5") is same as col("somecol") - 5 or expr("somecol") - 5
> Columns are just expressions.
> Columns and transformation of those columns compile to the same logical plan as parsed exprtession.
> Ex: expr("(((somecol + 5)*200)-6)")

#Access DF Columns
> spark.read.format("json").load("/data/flight-data/json_file_name").columns

#Dataframe Transformations:
> We can add/remove Row or Column
> We can transform row into column or viceversa
> We can change the order of rows based on the values in columns
> Creating dataframe:
df = spark.read.format("json").load("C:\\Users\\vivek.kumar\\Desktop\\Project\\DataSets\\spark\\flight\\2015-summary.json")
df.createOrReplaceTempView("dfTable")
> We can create dataframe on the fly by taking a set of rows and converting them as dataframe
myManualSchema = StringType([StructField("some",StringType(),True),StructField("col",StringType(),True),StructField("names",LongType(),False)])
myRow = Row("Hello",None,1)
myDF = spark.createDataFrame([myRow], myManualSchema)
myDF.show() 

#select and selectExpr:
> df.select("col1","col2").show(2)
  df.select(expr("col1 as column1")).show()

> selectExpr() is the most convenient way to perform transformation
df.selectExpr("DEST_COUNTRY_NAME as destination","ORIGIN_COUNTRY_NAME as origin","count").show(5)

> Expression(adding a new column) instead of all DF columns
df.selectExpr("*","(DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME) as withInCountry").show(5)
  
#Converting to Spark Types (Literals): we need to pass explicit values into Spark that are just a value (rather than a new column).
from pyspark.sql.functions import lit
df.select(expr("*"),lit(1).alias("one")).show(2)

#Adding Columns: There’s also a more formal way of adding a new column to a DataFrame, and that’s by using the withColumnmethod on our DataFrame.
df.withColumn("numberOne", lit(1)).show()
df.withColumn("withInColumn", expr("org_country == dest_country")).show(2)

#Renaming columns: This will rename the column with the name of the string in the first argument to the string in the second argument.
df.withColumnRenamed("dest_country", "dest").columns

#Case sensitivity: By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration.
-- in SQL set spark.sql.caseSensitive true

#Removing Columns;
df.drop("dest_country").columns
> We can drop multiple columns by passing in multiple columns as arguments: 
df.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")

#Changing a Column's Type(cast):
df.withColumn("longCount", col("count").cast("long"))

#Filtering Rows: There are two methods to perform this operation: you can use where or filter and they both will perform the same operation and accept the same argument types when used with DataFrames.
df.where("count < 2").show()
> Perform multiple conditions
df.where(col("count") < 2).where(col("org_dest") != "Croatia").show()

#Getting unique Rows: 
df.select("org_country","dest_country").distinct().count() -- Multiple columns as distinct selection
df.select("org_country").distinct().count() -- select count(distinct org_country) from tbl.

#Random Samples: Sometimes, you might just want to sample some random records from your DataFrame.
seed = 5 
withReplacement = False 
fraction = 0.5 
df.sample(withReplacement, fraction, seed).count()

#Random splits: Random splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. This is often used with machine learning algorithms to create training, validation, and test sets. In this next example, we’ll split our DataFrame into two different DataFrames by setting the weights by which we will split the DataFrame (these are the arguments to the function).

dataFrames = df.randomSplit([0.25, 0.75], seed) 
dataFrames[0].count() > dataFrames[1].count() # False

#Concatenating and Appending Rows (Union): To union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.
from pyspark.sql import Row 
schema = df.schema newRows = [ Row("New Country", "Other Country", 5L), Row("New Country 2", "Other Country 3", 1L) ] 
parallelizedRows = spark.sparkContext.parallelize(newRows) 
newDF = spark.createDataFrame(parallelizedRows, schema) 
df.union(newDF).where("count = 1").where(col("ORIGIN_COUNTRY_NAME") != "United States").show()

#Sorting Rows: When we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame. There are two equivalent operations to do this sort and orderBy that work the exact same way. They accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order.
df.sort("count").show(5) 
df.orderBy("count", "DEST_COUNTRY_NAME").show(5) 
df.orderBy(col("count"), col("DEST_COUNTRY_NAME")).show(5)

> To more explicitly specify sort direction, you need to use the asc and desc functions if operating on a column. These allow you to specify the order in which a given column should be sorted:
from pyspark.sql.functions import desc, asc 
df.orderBy(expr("count desc")).show(2) 
df.orderBy(col("count").desc(), col("DEST_COUNTRY_NAME").asc()).show(2) 
-- in 
SQL SELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2

#Limit: 
df.limit(10).show()

#Repartition and Coalesce:
> Another important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions. 

> Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns:

df.rdd.getNumPartitions() #1 
df.repartition(5)
df.repartition(5, col("DEST_COUNTRY_NAME"))

> Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):
df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2)

#Collecting Rows to the Driver:
collectDF = df.limit(10)
collectDF.take(5) # take works with an Integer count 
collectDF.show() # this prints it out nicely 
collectDF.show(5, False) 
collectDF.collect()


> Crosstab transformation: It is used to calculate the frequency across the tabs:
train.crosstab("Age", "Gender").show()
+----------+-----+------+
|Age_Gender|    F|     M|
+----------+-----+------+
|      0-17| 5083| 10019|
|     46-50|13199| 32502|
|     18-25|24628| 75032|
|     36-45|27170| 82843|
|       55+| 5083| 16421|
|     51-55| 9894| 28607|
|     26-35|50752|168835|
+----------+-----+------+

> """To create sample DF from base DF"""
train.sample(False, 0.2, 40).show(5)
print(train.sample(False, 0.2, 40).count())
train.sample(False, 0.3, 40).show(5)
print(train.sample(False, 0.3, 40).count())
train.sample(False, 0.5, 40).show(5)
print(train.sample(False, 0.5, 40).count())
