****************************************************Spark SQL*************************************************
DataFrames & DatSets:
---------------------
1: Creating DataFrames
2: Untyped Dataset Operations (aka DataFrame Operations)
3: Running SQL Queries Programmatically
4: Global Temporary View

5: Creating Datasets --> Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.

6: Interoperating with RDDs
> Inferring the Schema Using Reflection --> Spark SQL using two ways to create dataset from existing RDD: 1- Use Reflection to infer schema of RDD, 2- Create schema programatically and apply while creating the dataset.
> Programmatically Specifying the Schema
7: Aggregations
> Untyped User-Defined Aggregate Functions
> Type-Safe User-Defined Aggregate Functions
8: Data Sources
> Generic Load/Save Functions
 > Manually Specifying Options --> Dataset<Row> peopleJsonDF = spark.read().format("json").load("C:\\\\Users\\\\vivek.kumar\\\\Desktop\\\\Project\\\\DataSets\\\\spark\\\\Doc_example\\\\people.json");
 peopleJsonDF.write().format("parquet").save("C:\\\\Users\\\\vivek.kumar\\\\Desktop\\\\Project\\\\DataSets\\\\spark\\\\Doc_example\\\\peoplejsonparquet");
 > Run SQL on files directly --> spark.sql("select * from parquet.`C:\\Users\\vivek.kumar\\Desktop\\Project\\DataSets\\spark\\Doc_example\\users.parquet`").show
 
 > Save Modes --> Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an Overwrite, the data will be deleted before writing out the new data.
 
 > Saving to Persistent Tables --> DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command. Notice that an existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you. Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table.
 Note: To repair metdata of hive table: MSCK REPAIR 
 
 > Bucketing, Sorting and Partitioning
 
9: Parquet Files --> Parquet is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.

> Loading Data Programmatically

> Partition Discovery

> Schema Merging

> Hive metastore Parquet table conversion

 > Hive/Parquet Schema Reconciliation
 
 > Metadata Refreshing
 
10: Configuration

11: JSON Datasets

12: Hive Tables

13: Specifying storage format for Hive tables

14: Interacting with Different Versions of Hive Metastore

15: JDBC To Other Databases

16: Troubleshooting

17: Performance Tuning

> Caching Data In Memory

> Other Configuration Options

18: Distributed SQL Engine

> Running the Thrift JDBC/ODBC 
> Running the Spark SQL CLI
-----------------------------------------------------------
Note: We can create DF&DS from existing RDD, Hive tables, external sources.

DataFrame: It is a distributed collection of data organized into named columns.It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.

DatSet: A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).

Note: As mentioned above, in Spark 2.0, DataFrames are just Dataset of Rows in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.

Dataset creation:
> With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.

1: Dataset creation from existing RDD:
-----------------------------------
> Infering the schema using reflection --> Create RDD from file of java bean type i.e Person --> pass through map() function and set the values into Person objects --> create dataset of Row type by using rdd and Person.class as schema.

> Programmatically specifying the schema --> Create RDD from file of simple String type --> Pass schemastring into a string type variable --> create StructType schema by passing List of StructField into create structfield() method to create StructType schema --> Create Row type rdd by passing string type rdd through map using RowFactory class --> then call createdataframe method to create dataframe of Row type.

Note: While creating dataset , it needs seralized reflection of schema i.e encoded reflection.

2: Dataset creation from Hive or RDBMS tables:
----------------------------------------------

3: Dataset creation from spark data sources(files):
---------------------------------------------------

TempView: 
> we can create temp or global view from dataframe or dataset.
> Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1.

Encoders: Encoders are used to serialize the objects to process and store over the network.While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.

Optimization techniques: 
------------------------
Catalyst: It is a functional query optimization framework. It supports both rule based and cost based optimizatio techniques. We use Catalyst's general transformation in four phases:
1: Analysis : Spark SQL uses Catalyst rules and a Catalog object that tracks the tables in all data sources to resolve these attributes. 
2: Logical optimization : The logical optimization phase applies standard rule-based optimizations to the logical plan. And Cost-based optimization is performed by generating multiple plans using rules, and then computing their costs. These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, and other rules.
3: Physical planning : Spark SQL takes a logical plan and generates one or more physical plans, using physical operators that match the Spark execution engine. It then selects a plan using a cost model.At the moment, cost-based optimization is only used to select join algorithms: for relations that are known to be small, Spark SQL uses a broadcast join, using a peer-to-peer broadcast facility available in Spark. 
4: Code Generation : The final phase of query optimization involves generating Java bytecode to run on each machine.

Trees: 
> Tree contains node objects.
> Objects are immutable in nature.

Rules: We can manipulate tree using rules.
> we can define rules as function from one tree to another tree.

> 1: Rule based optimization(Catalyst is a modular library)

> 2: Cost based optimization 

Performance tuning:
-------------------
> Spark SQL can cache table by using an in-memory columnar format by calling spark.catalog.cacheTable("tableName") or dataframe.cache(). Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure.
Some properties:
spark.sql.inMemoryColumnarStorage.compressed = true
spark.sql.inMemoryColumnarStorage.batchSize = 10000
	
Four step update strategy with spark:
-------------------------------------

--# Hie update: 4 step update strategy
> Base table

--1: ingest data
drop table if exists base_tbl;
create table base_tbl(id int, field1 string, field2 string, modified_date string) row format delimited fields terminated by ',';
--Data:
insert into table base_tbl values(1, 'abcdef', 'efgh', '2014-03-01 09:22:52'),
(2, 'abc', 'efgh', '2014-03-01 09:22:52'),
(3, 'abcde', 'efgh', '2014-02-01 09:22:52');

drop table if exists inc_tbl;
create table inc_tbl(id int, field1 string, field2 string, modified_date string) row format delimited fields terminated by ',';
--Data:

insert into table inc_tbl values(1, 'abcdef', 'efgh', '2014-04-01 09:22:52'),
(2, 'abc', 'efgh', '2014-04-01 09:22:52'),
(3, 'abcde', 'e', '2014-03-01 10:22:52'),
(4, 'ab', 'ef', '2014-04-01 09:22:52'),
(5, 'abd', 'gh', '2014-05-01 09:22:52');


--2: Create Reconcile View
drop view if exists reconcile_view;
create view reconcile_view as
select t1.* from 
(select * from base_tbl
union all
select * from inc_tbl) t1
join
(select id, max(modified_date) max_modified from 
(select * from base_tbl
union all
select * from inc_tbl) t2
group by id)s
on t1.id == s.id 
and t1.modified_date = s.max_modified;

--3: Compact the view data into table
drop table if exists compact_tbl;
create table compact_tbl as
select * from reconcile_view;

--4: Purge the records into base table
drop table if exists base_tbl;
create table base_tbl as
select * from compact_tbl;