********https://www.youtube.com/watch?v=z7MtjZHeNhI&list=PLf0swTFhTI8q4pvjNTcjMPzCYPZIrpPAa**************main playlist
#Check the sqoop availibility
>sqoop
>sqoop help

#Connect with mysql database
>mysql -u retail_dba -h nn01.itversity.com -p
>Password:(Enter password)

#List-database command to check the database
sqoop list-databases --connect "jdbc:mysql://nn01.itversity.com:3306" --username retail_dba --password itversity

#To save the data and log of command
sqoop list-databases --connect "jdbc:mysql://nn01.itversity.com:3306" --username retail_dba --password itversity 1> list-database.data
sqoop list-databases --connect "jdbc:mysql://nn01.itversity.com:3306" --username retail_dba --password itversity 2> list-database.log

#To CRUD operation on RDBMS with sqoop
sqoop eval --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --query "select * from products limit 10"

#import command to load RDBMS table to HDFS: It will treat the given location as parent directory of the table and load the files into the same directory.
sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --target-dir "/user/vivekyadav/retail_db" 

#Import the table data based on the tablename itself inside the directory. It will create a new directory with the table name over the given directory.
sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table customers --warehouse-dir "/user/vivekyadav/retail_db"

#Import table with append --> Means data will append on the existing directory with table name and will not overwrite the existing content
--append
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --warehouse-dir "/user/vivekyadav/retail_db" --append

#This parameter for the sequence file format
--as-sequencefile
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table customers --warehouse-dir "/user/vivekyadav/retail_db" --as-sequencefile

#We can store the data in form of avro as well with using --as-avrodatafile parameter
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table customers --warehouse-dir "/user/vivekyadav/retail_db" --as-avrodatafile

#We have some avro-tools to convert avro format to json and vice-versa
>avro-tools tojson file_name.avro >> file_name.json

#If there is no primary key in table, need to use --split-by parameter
--split-by
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" --username retail_dba --password itversity --table order_items_vk --warehouse-dir "/user/vivekyadav/retail_db/retail_vk" --split-by order_item_id

OR 

#We can use -m,--num-mappers or --autoreset-to-one-mapper parameters
>>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" --username retail_dba --password itversity --table order_items_vk --warehouse-dir "/user/vivekyadav/retail_db/retail_vk" --num-mappers 1 or -m 1

>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" --username retail_dba --password itversity --table order_items_vk --warehouse-dir "/user/vivekyadav/retail_db/retail_vk" --autoreset-to-one-mapper

#To restrict the table record use --boundory-query
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" --username retail_dba --password itversity --table order_items_vk --warehouse-dir "/user/vivekyadav/retail_db/retail_vk" --boundory-query "select min(order_item_id),max(order_item_id) from order_items_vk where order_item_id not in (1000000)"


#create target directory by using --delete-target-dir
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --target-dir /user/vivekyadav/retail_db/order_items --delete-target-dir 

#Create output files in zip format using -z or --compress
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --target-dir /user/vivekyadav/retail_db/order_items --delete-target-dir -z

#Use of where clause inside the import command (We can use where clause with tablenot with query)
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --target-dir /user/vivekyadav/retail_db/order_items --delete-target-dir --where "order_item_id between 100 and 200"

#Import DBMS table into Hive 
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --hive-import --create-hive-table --hive-database vivekyadav --hive-table order_items

#Import specific columns from the DB table to hive table
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table order_items --columns "order_item_id,order_item_order_id,order_item_product_id"  --hive-database vivekyadav --hive-import --hive-overwrite --hive-table custom_order_item

#We can change data type of hive attributes while loading from DB by using --map-column-hive parameter
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table orders  --hive-database vivekyadav --hive-import --hive-overwrite --hive-table custom_orders --map-column-hive order_id=bigint

#We can set delimiter while importing the table into hive same as given in hive table creation
>create table orders () row format delimited fields terminated by '|'
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --table orders --target-dir "/apps/hive/warehouse/vivekyadav.db/orders" --append --fields-terminated-by '|'

#Saving the max attribute value into variable
>MAX_ORDER_ID=`sqoop eval --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --query "select max(order_id) from orders" 2>/dev/null|grep "^|"|grep -v max|awk -F " " '{print $2}'`

#Importing limited records in hive and then applying incremetal parameters to load delta
--check-column order_id
--incremental append
--last-value 60000
OR
we can use --where clause with append 

-----------------------------Incremental import to hive using sqoop-----------------------------
> We can use sqoop incremental import command with "-merge-key" option for updating the records in an already imported hive tables.
    # sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/employee_db" --username employee_dba --password --itversity
    --incremental lastmodified --merge-key employee_id
    --check-column emp_timestamp
    --target-dir /sqoop/emp_data

    > --incremental-lastmodified will import the updated and new records from RDBMS(MySql) database based on last latest value of emp_timestamp in hive.
    > --merge-key employee_id will flatten two datasets  into one , taking newest available records for each primary key (employee_id).
    > Example: Let’s assume there 500k records are there in Hive table. In incremental load, we got 100k new employee records and 50k records are updated employee records.
    Above sqoop command will import 150K records and using Merge tool it will append new records (100k) and update the 50k records based on primary key (employee_id).
    
--Acadguild approach
Step1: data in mysql table: employee
id,name,sal
1, a, 1000
2, b, 2000
3, c, 3000

sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306?emp_db" --username user --password pass --table employee --target-dir /home/emp_dir

Step2: Insert few more records into same table
id,name,sal
1, a, 1000
2, b, 2000
3, c, 3000
4, d, 4000
5, e, 5000
6, f, 6000

sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306?emp_db" --username user --password pass --table employee --target-dir /home/emp_dir
--incremental append --check-column id --last-value 3


        
-------------------------------------------------------------------------------------------------

#Import all tables by using --import-all-tables
>sqoop import-all-tables --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --warehouse-dir "/user/vivekyadav/import_all_tables" --exclude-tables "departments,categories" --as-avrodatafile

#Importing all tables into hive 
>sqoop import-all-tables --connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" --username retail_dba --password itversity --hive-database vivekyadav --hive-import --hive-overwrite

#Exercise : 12 for importing the data to HDFS and hive tables.
database : nyse
-->Import nyse data to HDFS
>sqoop import --connect "jdbc:mysql://nn01.itversity.com:3306/nyse" --username nyse_ro --password itversity --table stocks_eod --hive-import --hive-database vivekyadav  --hive-overwrite --split-by  tradedate -m 8 -z

#Export the HDFC data into DBMS
>sqoop export --connect "jdbc:mysql://nn01.itversity.com:3306/retail_export" \
--username retail_dba \
--password itversity \
--export-dir "/user/vivekyadav/import_all_tables/orders" \
--table vi_orders

#To check the occupied space by file in hdfs
>hadoop fs -du -s(summarize) -h(human understandable) /user/vivekyadav/import_all_tables/orders
