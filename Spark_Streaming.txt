************************************************Spark Streaming***********************************************************
Spark Streaming: 
> Spark Streaming is an extension of the core Spark API that enables scalable,  high-throughput, fault-tolerant stream processing of live data streams.
> Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.
> Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.
> Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees.
> However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees.
> Instead of processing the streaming data one record at a time, Spark Streaming discretizes the streaming data into tiny, sub-second micro-batches.


Stream Processing Architecture - Micro Batching:
> Spark Streaming uses a “micro-batch” architecture, where the streaming computation is treated as a continuous series of batch computations on small batches of data.
> New batches are created at regular time intervals. The size of the time interval is called “Batch Interval”
> Each input batch forms an RDD, and is processed using Spark jobs to create
 other RDDs.
> Recieve: Recieve streaming data from several resources e.g live logs, system telemetry data, IOT devices data etc.
> Process: Process the parallel on cluster.
> Output: Output the results out to downstream systems like HBase, Casandra, Cafka etc.
> Streaming computation is treated as continuous series of batch computations on small batches of data.
> New batches are created at regular time intervals. The size of interval called Batch Interval.
> Each input batch forms an RDD, and is processed using spark jobs to create other RDDs.

Dynamic load balancing:
> Traditional system: Unevenly partitioned streams, bottleneck node, static scheduling  of continuous operators to nodes can cause bottlenecks.
> Spark streaming: Tasks scheduled based on the available resources, dynamic scheduling of tasks ensures even distribution of load.
> Spark tasks are assigned dynamically to the workers based on the locality of the data and available resources.

Discretized Streams(DStreams):
> Spark DStream (Discretized Stream) is the basic abstraction of Spark Streaming. DStream is a continuous stream of data. It receives input from various sources like Kafka, Flume, Kinesis, or TCP sockets. It can also be a data stream generated by transforming the input stream.
> DStream is the sequence of data arriving over time.
> Internally, each DStream is represented as a sequence of RDDs arriving at each time step.

Fast failure and straggler recovery:
> Traditional system: failed stream replayed to new node, slower recovery by using single node for recomputation.
> Spark streaming: Failed tasks are relaunched on all other nodes, faster recovery using multiple node for recomputation.
> In Spark, the computation is already discretized into small, deterministic tasks that can run anywhere without affecting correctness.

Discretized streams:
> Discretized Stream or DStream is the basic abstraction provided by Spark Streaming.
> A DStream is a sequence of data arriving over time.
> Internally, each DStream is represented as a sequence of RDDs arriving at each time
  step (hence the name “Discretized”).
> DStreams can be created from various input sources such as Flume, Kafka, or HDFS.

Transformations & Output Operations:
> Once DStream built, it offers two types of operations:
 > Transformations
 > Output operations
> DStreams provide many of the same operations available on RDDs, plus new operations related to time, such as sliding windows.

Transformations:
> Transformations on DStreams can be grouped into either Stateless or Stateful:
 > Stateless Transformations:
  > Simple RDD transformations being applied on every batch—that is, every RDD in a Dstream, for example – map, filter, reduceByKey, flatMap etc.

 > Stateful Transformations:
  > Stateful transformations are operations on DStreams that track data across time; that is, some data from previous batches is used to generate the results for a new batch.
  > The two main types are:
   > 1- Windowed operations: which act over a sliding window of time periods. Ex: window(), countByWindow(), reduceByWindow(), reduceByKeyandWindow(), countByValueAndWindow()
   > 2- updateStateByKey(): which is used to track state across events for each key.
 
Output operations:
> Output operations specify what needs to be done with the final transformed data in a stream (e.g., pushing it to an external database or printing it to the screen). 
> Example: print(), saveAsTextFile(), foreachRDD() etc.

  
updateStateByKey Operations:
> The updateStateByKey operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.
  > Define the state - The state can be an arbitrary data type.
  > Define the state update function - Specify with a function how to update the state using the previous state and the new values from an input stream.

24/7 Operations: There are some special steps for 24/7 spark streaming.
> Checkpointing: There are two types of data that are checkpointed.
  > Metadata checkpointing: Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later).
  > Data Checkpointing: Saving of the generated RDDs to reliable storage. This is necessary in some stateful transformations that combine data across multiple batches. 

Driver Fault Tolerance: 
> Tolerating failures of the driver node requires a special way of creating our Streaming Context, which takes in the checkpoint directory. Instead of simply calling new StreamingContext, we need to use the StreamingContext.getOrCreate() function. 

Worker Fault Tolerance: 
> For failure of a worker node, Spark Streaming uses the same techniques as Spark for its fault tolerance.
> All the data received from external sources is replicated among the Spark workers.
> All RDDs created through transformations are also fault tolerant as they can be recreated either using Data Checkpointing or lineage graph.

Receiver Fault Tolerance:
> In such a failure, Spark Streaming restarts the failed receivers on other nodes in the cluster. However, whether it loses any of the received data depends on the nature of the source (whether the source can resend data or not) and the implementation of the receiver (whether it updates the source about received data or not).

Zero Data Loss & Spark Consumers:
> Reciever based approach: 
In Apache Spark 1.2, preliminary support for write ahead logs (also known as journaling)  was added to Spark Streaming to improve this recovery mechanism and give stronger guarantees of zero data loss for more data sources. 
> Pros:  
WAL design could work with non-Kafka data sources

>Cons: 
Long running receivers make parallelism awkward and costly. It forces you to 
create multiple DStreams to increase the throughput. 

Achieving zero-data loss in the first approach required the data to be stored in a 
Write Ahead Log, which further replicated the data. This is actually inefficient 
as the data effectively gets replicated twice - once by Kafka, and a second time by the 
Write Ahead Log. 

Note: There is a small chance some records may get consumed twice under some failures. 

> Direct Approach(No Receivers):
  > It simply decides at the beginning of every batch interval what is the range of offsets to consume per partition. Later, when each batch’s jobs are executed, the data corresponding to the offset ranges is read from Kafka for processing
  > Pros:  
Spark partition 1:1 Kafka topic/partition, easy parallelism
No duplicate writes
No dependency on HDFS

  > Cons: 
Specific to Kafka
Need adequate Kafka retention (OffsetOutOfRange is your fault)

Note: That being said, if you use the Direct Stream approach, you have 2 options to avoid data losses:
Using checkpoints
Keeping track of the offsets that have been processed. 

Kafka commands:
./kafka-topics.sh --create --replication-factor 3 --partitions 13 --topic my-example-topic --zookeeper nn01.itversity.com:2181

./kafka-topics.sh --list --zookeeper localhost:2181  

./create-topic.sh

--Execute sample program in cluster:
nc -lk 9999
spark-submit --class streaming_unix.StreamingWordCount --master local --conf spark.ui.port=4444 /home/ubuntu/vivek/jars/newspark-0.0.1-SNAPSHOT.jar

spark-submit --class streaming_unix.WordCountWithWindow --master local --conf spark.ui.port=4444 /home/ubuntu/vivek/jars/newspark-0.0.1-SNAPSHOT.jar

C:\Users\Public\spark\bin\spark-submit --class sparkrddexamples.SparkTransformationExamples --master local --conf spark.ui.port=4444 java_spark-0.0.1-SNAPSHOT.jar
/newspark/src/main/java/streaming_unix/StreamingWordCount.java
**************************************************************************************************************************

Examples Chapter 1&2: 
---------------------

1: Word Count:
gtRDD = sc.textFile("file:///home/impetus/Desktop/spark_home/input/greetings.txt")
gtWords = gtRDD.flatMap(lambda line: line.split(" "))
gtWordsOne = gtWords.map(lambda word: (word,1))
wordCount = gtWordsOne.reduceByKey(lambda a,b: a+b)
wordCount.saveAsTextFile("file:///home/impetus/Desktop/spark_home/output")

val a= sc.textFile("file:///home/impetus/Desktop/spark_home/input/greetings.txt").flatMap(x=> x.split(" ")).map(x=> (x,1)).reduceByKey(x,y => (x+y))

2: read csv and perform transformations:

flightData2015 = spark.read.option("inferSchema","true").option("header","true").csv("C:\\Users\\vivek.kumar\\Desktop\\Project\\DataSets\\spark\\flight\\2015-summary.csv")
+-----------------+-------------------+-----+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
+-----------------+-------------------+-----+
|    United States|            Romania|   15|
|    United States|            Croatia|    1|
|    United States|            Ireland|  344|
|            Egypt|      United States|   15|
|    United States|              India|   62|
+-----------------+-------------------+-----+
flightData2015.sort("count").explain()
spark.conf.set("spark.sql.shuffle.partitions","5")
flightData2015.sort("count").take(2)
flightData2015.createOrReplaceTempView("flight_data_2015")

sqlWay = spark.sql("""select DEST_COUNTRY_NAME,count(1) from flight_data_2015 group by DEST_COUNTRY_NAME""")
dataframeWay = flightData2015.groupBy("DEST_COUNTRY_NAME").count()

>>> sqlWay.explain()
== Physical Plan ==                                                                                                                                                                                                                           
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])                                                                                                                                                                         
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)                                                                                                                                                                                         
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])                                                                                                                                                           
      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/vivek.kumar/Desktop/Project/DataSets/spark/flight/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
                                                    
>>> dataframeWay.explain()                                                                                                                                                                                                                    
== Physical Plan ==                        
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])                                                                                                                                                                         
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10,5)                                                                                                                                                                                         
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])                                                                                                                                                           
      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/vivek.kumar/Desktop/Project/DataSets/spark/flight/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>                                                                                                                                                                                       

spark.sql("""select max(count) from flight_data_2015""").show(5)
+----------+ 	  
|max(count)| 
+----------+ 
|    370002| 
+----------+ 

from pyspark.sql.functions import max
flightData2015.select(max("count")).show(1)
+----------+  
|max(count)|  
+----------+  
|    370002|  
+----------+  

maxSql = spark.sql("""select DEST_COUNTRY_NAME,sum(count) as destination_total from flight_data_2015 group by DEST_COUNTRY_NAME order by sum(count) desc limit 5""")
maxSql.show()
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+


from pyspark.sql.functions import desc
maxDf = flightData2015.groupBy("DEST_COUNTRY_NAME").sum("count").withColumnRenamed("sum(count)","destination_total").sort(desc("destination_total")).limit(5).show()
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+

Examples Chapter-5:
-------------------
Basic structured operations

#Read data from json file
df = spark.read.format("json").load("C:\\Users\\vivek.kumar\\Desktop\\Project\\DataSets\\spark\\flight\\2015-summary.json")
df.printSchema

from pyspark.sql.functions import col. column
col("num")
column("num")

#create temp view or table from dataframe
df.createOrReplaceTempView("dfTable")

#Create the DF on fly using manual schema
spark.conf.set("spark.sql.shuffle.partitions","5")
flightData2015.sort("count").take(2)

myManualSchema = StructType([
    StructField("some",StringType(),True),
    StructField("col",StringType(),True),
    StructField("names",LongType(),True)
    ])

myRow = Row("Hello","world",1)

myDF = spark.createDataFrame([myRow],myManualSchema)
myDF.show()


#Use of select and selectExpr method()
 df.select("DEST_COUNTRY_NAME").show(2)
 df.select("DEST_COUNTRY_NAME","ORIGIN_COUNTRY_NAME","count").show(5)
 
 from pyspark.sql.functions import expr
 df.select(expr("DEST_COUNTRY_NAME as destination")).show(5)
 
df.selectExpr("DEST_COUNTRY_NAME as destination","DEST_COUNTRY_NAME").show(5)
df.selectExpr("*","(DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME) as withInCountry").show(5)

#Adding column
from pyspark.sql.functions import lit
df.withColumn("one",lit(1)).show(5)

#Renaming column
df.withColumnRenamed("DEST_COUNTRY_NAME","Destination").columns

#Drop the dedicated column
df.drop("DEST_COUNTRY_NAME").columns

#Filter the records using filter() method
df.filter(col("count")>2).show(2)

#Filter with multiple conditions
df.where(col("count")>2).where(col("DEST_COUNTRY_NAME")!= "United States").show(5)

#Submit Java application using spark-submit
./bin/spark-submit \
  --class sparkrddexamples.SparkTransformationExamples.java \
  --master local[8] \
  /path/to/examples.jar \
  100

#Command to run in yarn mode
--Add these properties
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
  
/usr/local/spark/bin/spark-submit --class sparkrddexamples.SparkTransformationExamples --master yarn /home/impetus/Desktop/spark_jars/java_spark-0.0.1-SNAPSHOT.jar


#To see logs from yarn
yarn logs -applicationId application_1473860344791_0001

#Command to run in windows
C:\Users\Public\spark\bin\spark-submit --class sparkrddexamples.SparkTransformationExamples --master local --conf spark.ui.port=4444 java_spark-0.0.1-SNAPSHOT.jar


C:\Users\Public\spark\bin\spark-submit --class sparkrddexamples.WordCountWithWindow --master local --conf spark.ui.port=4444 java_spark-0.0.1-SNAPSHOT.jar
